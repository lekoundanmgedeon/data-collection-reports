{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLdljxft6EW7"
      },
      "source": [
        "## LEKOUNDA NGOLO Mardochet Gedeon\n",
        "### COOP MSC | DATA SCIENCE\n",
        "### ASSIGNMENT 4 : DATA COLLECTION\n",
        "#### 29-111-2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j67veSSy_lN1"
      },
      "outputs": [],
      "source": [
        "url = 'https://ci.coinafrique.com/categorie/appartements?page=13'\n",
        "# scrape data (number of rooms, number of bathroom, area, price, location, image_link) over 200 pages\n",
        "# using selenium combined with BeautifulSoup\n",
        "# Clean the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1oc1cp0im-IY"
      },
      "outputs": [],
      "source": [
        "#!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DTKy31-E6CYt"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4FFvJK6NAWZ9"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import pandas as pd\n",
        "import time\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "Srnf2ks6752Z",
        "outputId": "c58582d3-0172-4a74-9a9e-db316bd9dfcc"
      },
      "outputs": [],
      "source": [
        "# instantiate a Chrome options object\n",
        "options = webdriver.ChromeOptions()\n",
        "# set the options to use Chrome in headless mode (used for running the script in the background)\n",
        "options.add_argument(\"--headless=new\")\n",
        "# initialize an instance of the Chrome driver (browser) in headless mode\n",
        "driver = webdriver.Chrome(options=options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "L5KkDDPuAVHO"
      },
      "outputs": [],
      "source": [
        "base_url = \"https://ci.coinafrique.com/categorie/appartements?page=\"\n",
        "data = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J3XxZ6SvAuUJ"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_page(page_number):\n",
        "    url = f'https://ci.coinafrique.com/categorie/appartements?page={page_number}'\n",
        "    driver.get(url)\n",
        "    time.sleep(3)\n",
        "\n",
        "    soup = bs(driver.page_source, \"html.parser\")\n",
        "\n",
        "    # REAL CSS CLASS FOR EACH AD PREVIEW\n",
        "    containers = soup.find_all(\"div\", class_=\"card ad__card\")\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for container in containers:\n",
        "        try:\n",
        "            # Extract product link\n",
        "            a_tag = container.find(\"a\", href=True)\n",
        "            if not a_tag:\n",
        "                continue\n",
        "\n",
        "            product_url = \"https://ci.coinafrique.com\" + a_tag[\"href\"]\n",
        "\n",
        "            # Open detailed page\n",
        "            driver.get(product_url)\n",
        "            time.sleep(2)\n",
        "            ## Mr ABDUL W. time.sleep(2) ceci est extremement necessaire pour les requetes \n",
        "            ## en effet ici on donne un delais de 2s pour chaque requetes, cela permettra de \n",
        "            ## recuperer un maximum nombre de page ainsi evitera de surcharger le serveur\n",
        "\n",
        "            soup_detail = bs(driver.page_source, \"html.parser\")\n",
        "\n",
        "            # Title\n",
        "            title_tag = soup_detail.find(\"h1\")\n",
        "            title = title_tag.get_text(strip=True) if title_tag else None\n",
        "            # On revoie None comme valeur par defaut dans le cas ou on arrive pas a recuperer le titer\n",
        "            # Price\n",
        "            price_tag = soup_detail.find(\"p\", class_=\"ad__price\")\n",
        "            if not price_tag:\n",
        "                price_tag = soup_detail.find(\"span\", class_=\"price\")\n",
        "\n",
        "            if price_tag:\n",
        "                price_raw = price_tag.get_text(strip=True)\n",
        "                price = price_raw.replace(\"CFA\", \"\").replace(\" \", \"\")\n",
        "            else:\n",
        "                price = None\n",
        "\n",
        "            # full adress\n",
        "            loc_tag = soup_detail.find(\"p\", class_=\"ad__card-location\")\n",
        "            if loc_tag:\n",
        "                location = loc_tag.get_text(strip=True)\n",
        "            else:\n",
        "                # backup\n",
        "                loc2 = soup_detail.find(\"span\", class_=\"location\")\n",
        "                location = loc2.get_text(strip=True) if loc2 else None\n",
        "\n",
        "            # ==============================\n",
        "            # Extract Rooms / Bathrooms\n",
        "            # ==============================\n",
        "            rooms = None\n",
        "            bathrooms = None\n",
        "\n",
        "            info_list = soup_detail.find_all(\"li\", class_=\"center\")\n",
        "\n",
        "            for li in info_list:\n",
        "                label = li.find_all(\"span\")\n",
        "                if len(label) >= 2:\n",
        "                    name = label[0].get_text(strip=True).lower()\n",
        "                    qt = label[1].get_text(strip=True)\n",
        "\n",
        "                    if \"pièces\" in name:\n",
        "                        rooms = qt\n",
        "                    elif \"bain\" in name:\n",
        "                        bathrooms = qt\n",
        "            \n",
        "            # ==============================\n",
        "            # Extract first main image\n",
        "            # ==============================\n",
        "            img_tag = soup_detail.find(\"img\")\n",
        "            image_link = img_tag[\"src\"] if img_tag else None\n",
        "\n",
        "            data.append({\n",
        "                \"title\": title,\n",
        "                \"price\": price,\n",
        "                \"location\": location,\n",
        "                \"rooms\": rooms,\n",
        "                \"bathrooms\": bathrooms,\n",
        "                \"image\": image_link,\n",
        "                \"url\": product_url\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"Error on item:\", e)\n",
        "            continue\n",
        "    print(len(containers))\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping page 1\n",
            "0\n",
            "Scraping page 2\n",
            "0\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "Nombre total d'entrées: 0\n"
          ]
        }
      ],
      "source": [
        "datas = [] # liste vide \n",
        "\n",
        "for page in range(1, 3):\n",
        "    print(f\"Scraping page {page}\")\n",
        "    page_data = scrape_page(page)\n",
        "    datas.extend(page_data)\n",
        "\n",
        "driver.quit()\n",
        "\n",
        "df = pd.DataFrame(datas)\n",
        "print(df.head())\n",
        "print(\"Nombre total d'entrées:\", len(df))\n",
        "\n",
        "#df.to_csv(\"coinafrique_apartments_raw.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "daily-notebook",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
